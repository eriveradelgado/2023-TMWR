---
title: "Chapter-9"
author: "Edgardo Rivera-Delgado"
format: html
editor: visual
---

# Notes

Necessary objects to complete the chapter

```{r}
#|results: hide
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

## Judging Model Effectiveness

Performance depends on the application of the model (similar to the inference, prediction, description point made in earlier chapters). In prediction, the metrics are related to "predictive strength or how close the predictions are to the observed data" (Verbatim).

```{r}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
```

```{r}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()+
  theme_bw()


```

Calculating performance metrics for regression models with yardstick involves passing a data frame to the function and specifying which column is the truth and which is the model estimate as shown below.

```{r}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
rsq(ames_test_res, truth = Sale_Price, estimate = .pred)
mae(ames_test_res, truth = Sale_Price, estimate = .pred)
```

Multiple metrics can be computed with function *metric_set()*

```{r}
ames_metrics <- metric_set(rmse, mae, rsq)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```

## Binary Classification Metrics

```{r}
data("two_class_example")
tibble(two_class_example)
```

```{r}
conf_mat(two_class_example, truth = truth, estimate = predicted)
accuracy(two_class_example, truth = truth, estimate = predicted)
mcc(two_class_example, truth = truth, estimate = predicted)
f_meas(two_class_example, truth = truth, estimate = predicted)

classification_metrics <- metric_set(accuracy, mcc, f_meas)
```

mcc: matthews correlation coefficient quality of both positive and negative examples

f1: emphasizes event of interest

*event_level()*: Used to distinguish negative and positive events inside of yardstick classification metric functions.

In yardstick the first level is the event of interest

For classification there are metrics that use the probabilities instead of the outcomes. Such is the case for ROC curves.

```{r}
roc <- roc_curve(two_class_example, truth, Class1)
roc_auc(two_class_example, truth, Class1)
roc %>% 
  autoplot()
```

## Multiclass Classification Metrics

```{r}
data("hpc_cv")
tibble(hpc_cv)
```

```{r}
accuracy(hpc_cv, truth = obs, estimate = pred)
mcc(hpc_cv, truth = obs, estimate = pred)
```

Some techniques to measure model performance can be extended from binary classification metrics into multiclass classification.

*micro-averaging compute contribution for each class, aggregate then compute a single metric*

*macro-averaging one class versus all class averaging*

*macro weighted averaging one class versus all class taking account the sample size of each class*

These are called *estimators* inside of yardstick.

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L)

roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro")
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "hand_till")

```

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred) %>% 
  ggplot(aes(y = .estimate, x = ""))+
  geom_point()+
  stat_summary(geom = "pointrange", fun.data = mean_cl_boot, size = 1, color = "red")
```

Obtaining roc curves for each class can be done by passing roc_curve to autoplot.

```{r}
# Four 1-vs-all ROC curves for each fold
hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```
